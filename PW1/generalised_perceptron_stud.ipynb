{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "expressed-suffering",
   "metadata": {},
   "source": [
    "## Generalised Perceptron\n",
    "\n",
    "Binary classification based on MNIST and Fashion MNIST data. \n",
    "\n",
    "It restricts the classification problem to two classes, selects them from the (Fashion-)MNIST dataset, splits it up into a train and test part, does normalisation and then trains a binary classification (logistic regression) to learn to differentiate between the two categories.\n",
    "\n",
    "Both datasets consist of images with 28x28 = 784 pixel each. The features refer to these pixel values of the images.\n",
    "\n",
    "You can choose MNIST or Fashion-MNIST data in cell [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import read_data, plot_img, plot_tiles, plot_error, plot_cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9ac65-cf2d-4571-93ce-aeb172cbfbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, labels_map = read_data('fashionMNIST') #MNIST or fashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed1556-c895-4ab6-be00-c0bc4a660abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "#append rows x cols tiles of images\n",
    "rows = 8\n",
    "cols = 18\n",
    "#figure size can be set\n",
    "fig_size = [8,8]\n",
    "\n",
    "plot_tiles(x, rows, cols, fig_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose a given class 0..9\n",
    "digit  = 0\n",
    "\n",
    "plot_tiles(x[y == digit,:], rows, cols, fig_size)\n",
    "print(labels_map[digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e7f320-89df-4cd2-a795-1b4fcee72901",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the two classes for your training and test set, select train and test split and to normalization\n",
    "def prepare_data(digit_1, digit_2, train_size=0.8, min_max_normalise=1, flatten=1):\n",
    "    \"\"\"\n",
    "    prepare the data for training\n",
    "\n",
    "    Arguments:\n",
    "    digit_1 -- first digit ('True') to select\n",
    "    digit_2 -- second digit ('False') to select\n",
    "    train_size -- fraction of train image size\n",
    "    min_max_normalise -- whether to do min-max-normalisation (1) or rescaling (0)\n",
    "    flatten -- whether to flatten the 28x28 image to single row\n",
    "    \"\"\"\n",
    "    \n",
    "    #select the digit\n",
    "    x_sel_1 = x[y == digit_1,:]\n",
    "    x_sel_2 = x[y == digit_2,:]\n",
    "\n",
    "    #append the x data\n",
    "    x_sel = np.append(x_sel_1, x_sel_2, 0)\n",
    "    #construct y-data (digit_1 is 'True')\n",
    "    y_sel = np.append(np.ones((x_sel_1.shape[0],1)),\n",
    "                      np.zeros((x_sel_2.shape[0],1)), 0)\n",
    "\n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "    #do train and test split, also take care to randomly shuffle the data (why?)\n",
    "    num_samples = x_sel.shape[0]\n",
    "    max_train_ind = int(0.5*num_samples)\n",
    "    \n",
    "    x_train = x_sel[:max_train_ind]\n",
    "    x_test = x_sel[max_train_ind:]\n",
    "    \n",
    "    y_train = y_sel[:max_train_ind]\n",
    "    y_test = y_sel[max_train_ind:]\n",
    "\n",
    "    ### END YOUR CODE ###     \n",
    " \n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "    #implement min-max-normalisation and rescaling, take care of converting data type to float!  \n",
    "    if min_max_normalise:\n",
    "        x_train = x_train.astype(float)\n",
    "        x_test = x_test.astype(float) \n",
    "    else:\n",
    "        x_train = x_train.astype(float) \n",
    "        x_test = x_test.astype(float)\n",
    "\n",
    "    ### END YOUR CODE ###       \n",
    "\n",
    "    if flatten:\n",
    "        m = x_train.shape[0]\n",
    "        x_train = x_train.reshape([m,-1])\n",
    "        m = x_test.shape[0]\n",
    "        x_test = x_test.reshape([m,-1])\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bcd5fb-d25e-4fe5-906e-386c955898a1",
   "metadata": {},
   "source": [
    "### Class NeuralNetwork\n",
    "\n",
    "This class constructs a generalised perceptron. Cost function can be either MSE or CE (chosen in constructor). The method $propagate()$ returns the prediction $$ \\hat{y}^{(i)}=h_\\theta(\\mathbf{x}^{(i)}) $$ on the input data (can be a n x 784 matrix of n images) and $back\\_propagate()$ determines the gradients of the cost function with respect to the parameters (weights and bias) $$ \\nabla_{\\mathbf{\\theta}} J(\\mathbf{\\theta}) $$\n",
    "The method $gradient\\_descend()$ finally does the correction of the parameters with a step in the negative gradient direction, weighted with the learning rate $$\\alpha$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4c21d2-2815-4e6c-b359-6b0f26eac64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    NN class handling the layers and doing all propagation and back-propagation steps\n",
    "    \"\"\"\n",
    "    cost_MSE = 0\n",
    "    cost_CE = 1\n",
    "    \n",
    "    def __init__(self, cost_function=cost_MSE, random_std = 0, size_in = 784):\n",
    "        \"\"\"\n",
    "        constructor\n",
    "\n",
    "        Arguments:\n",
    "        cost_function -- can be cost_MSE (0, default) or cost_CE (1)\n",
    "        random_std -- std for initialisation of weight (default is 0)\n",
    "        size_in -- size of input image\n",
    "        \"\"\"\n",
    "        self.size_in = size_in\n",
    "\n",
    "        self.cost_function = cost_function\n",
    "        \n",
    "        #initialize weights and bias (zero or random)\n",
    "        self.initialise_weights(random_std)\n",
    "        \n",
    "        # result array for cost and error of training and test set\n",
    "        self.result_data = np.array([])\n",
    "        self.result_data_dL = np.array([])\n",
    "        \n",
    "        #we keep a global step counter, thus that optimise can be called \n",
    "        #several times with different settings\n",
    "        self.epoch_counter = 0 \n",
    "\n",
    "        \n",
    "    def initialise_weights(self, random_std):\n",
    "        \"\"\"\n",
    "        initialize weights and bias (if random_std == 0 all weights are zero)\n",
    "        \"\"\" \n",
    "        self.w = random_std*np.random.randn(self.size_in,1)\n",
    "        self.b = 0\n",
    "    \n",
    "    \n",
    "    def propagate(self, x):\n",
    "        \"\"\"\n",
    "        predicted outcome for x\n",
    "        \"\"\"\n",
    "        z = x @ self.w + self.b\n",
    "        y_pred = self.activation_function(z)\n",
    "                    \n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "    def activation_function(self, z):\n",
    "        \"\"\"\n",
    "        apply activation function\n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ###\n",
    "        \n",
    "        return z\n",
    "    \n",
    "        ### END YOUR CODE ###   \n",
    "    \n",
    "    \n",
    "    def back_propagate(self, x, y_pred, y):\n",
    "        \"\"\"\n",
    "        calculates the gradients of cost function wrt w and b\n",
    "        \"\"\"\n",
    "        #abbreviation\n",
    "        m = x.shape[0]\n",
    "        \n",
    "        if self.cost_function == self.cost_MSE:\n",
    "        ### START YOUR CODE ###\n",
    "        \n",
    "            self.grad_w = np.zeros((784,1))\n",
    "            self.grad_b = 0\n",
    "        else:  \n",
    "            self.grad_w = np.zeros((784,1))\n",
    "            self.grad_b = 0\n",
    "            \n",
    "        ### END YOUR CODE ###   \n",
    "    \n",
    "    \n",
    "    def gradient_descend(self, alpha):\n",
    "        \"\"\"\n",
    "        applies gradient descend step to w and b\n",
    "        \"\"\"\n",
    "        self.w -= alpha * self.grad_w\n",
    "        self.b -= alpha * self.grad_b\n",
    "    \n",
    "    \n",
    "    def calc_error(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        get error information\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        \n",
    "        res = np.round(y_pred)        \n",
    "        error = np.sum(np.abs(res - y)) / m \n",
    "\n",
    "        return error\n",
    "    \n",
    "    \n",
    "    def cost_funct(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        calculates the cost function\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        \n",
    "        if self.cost_function == self.cost_MSE:\n",
    "            \n",
    "            ### START YOUR CODE ###\n",
    "        \n",
    "            cost = 0.123\n",
    "        else:\n",
    "            cost = 0.123       \n",
    "            \n",
    "        ### END YOUR CODE ###                     \n",
    "                            \n",
    "        return cost   \n",
    "    \n",
    "    \n",
    "    def append_result(self):\n",
    "        \"\"\"\n",
    "        append cost and error data to output array\n",
    "        \"\"\"\n",
    "        # determine cost and error functions for train and test data\n",
    "        y_pred_train = self.propagate(self.data['x_train'])\n",
    "        y_pred_test = self.propagate(self.data['x_test'])\n",
    "\n",
    "        res_data = np.array([[self.cost_funct(y_pred_train, self.data['y_train']), \n",
    "                              self.calc_error(y_pred_train, self.data['y_train']),\n",
    "                              self.cost_funct(y_pred_test, self.data['y_test']), \n",
    "                              self.calc_error(y_pred_test, self.data['y_test'])]])\n",
    "        \n",
    "        # first call\n",
    "        if self.result_data.size == 0:\n",
    "            self.result_data = res_data\n",
    "        else:\n",
    "            self.result_data = np.append(self.result_data, res_data, 0)\n",
    "\n",
    "        #increase epoch counter here (used for plot routines below)\n",
    "        self.epoch_counter += 1 \n",
    "        \n",
    "        return res_data\n",
    "    \n",
    "    \n",
    "          \n",
    "    def optimise(self, data, epochs, alpha, debug=0):\n",
    "        \"\"\"\n",
    "        performs epochs number of gradient descend steps and appends result to output array\n",
    "\n",
    "        Arguments:\n",
    "        data -- dictionary with NORMALISED data\n",
    "        epochs -- number of epochs\n",
    "        alpha -- learning rate\n",
    "        debug -- False (default)/True; get info on gradient descend step\n",
    "        \"\"\"\n",
    "        #access to data from other methods\n",
    "        self.data = data\n",
    "        \n",
    "        # save results before 1st step\n",
    "        if self.epoch_counter == 0:\n",
    "            res_data = self.append_result()\n",
    "\n",
    "        for i0 in range(0, epochs):            \n",
    "            y_pred = self.propagate(data['x_train'])\n",
    "            self.back_propagate(data['x_train'], y_pred, data['y_train'])\n",
    "            self.gradient_descend(alpha)\n",
    "          \n",
    "            res_data = self.append_result()\n",
    "                      \n",
    "            if debug and np.mod(i0, debug) == 0:\n",
    "                print('result after %d epochs, train: cost %.5f, error %.5f ; test: cost %.5f, error %.5f'\n",
    "                                          % (self.epoch_counter-1, res_data[0, 0], res_data[0, 1], res_data[0, 2], res_data[0, 3]))\n",
    "\n",
    "        if debug:\n",
    "            print('result after %d epochs, train: cost %.5f, error %.5f ; test: cost %.5f, error %.5f'\n",
    "                  % (self.epoch_counter-1, res_data[0, 0], res_data[0, 1], res_data[0, 2], res_data[0, 3]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcfae34-9c4a-461f-b06f-d9abc093b775",
   "metadata": {},
   "source": [
    "### Sample execution of Neural Network\n",
    "\n",
    "The cell below shows how to use the class NeuralNetwork and how to perform the optimisation. The training and test data is given as dictionary in the call to the method $optimise()$. This method can be called several times in a row with different arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5e5245-456f-4026-8978-8a1b8776ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = prepare_data(0, 2, train_size=0.8, min_max_normalise=0, flatten=1)\n",
    "\n",
    "data = {'x_train' : x_train, 'y_train' : y_train, 'x_test' : x_test, 'y_test' : y_test}\n",
    "\n",
    "\n",
    "NNet = NeuralNetwork(cost_function=1, random_std = 0.2, size_in = 784)\n",
    "\n",
    "NNet.optimise(data, 500, 0.5, debug=100)\n",
    "NNet.optimise(data, 500, 0.25, debug=100)\n",
    "\n",
    "plot_error(NNet)\n",
    "plot_cost(NNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b7882-711e-4aea-8e93-6aedd179c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse false classified training or test images\n",
    "y_pred = np.round(NNet.propagate(data['x_test']))\n",
    "false_classifications = data['x_test'][(y_pred != data['y_test'])[:,0]]\n",
    "\n",
    "print(false_classifications.shape)\n",
    "\n",
    "#append rows x cols tiles of digits\n",
    "rows = 11\n",
    "cols = 11\n",
    "#figure size can be set\n",
    "fig_size = [8,8]\n",
    "\n",
    "plot_tiles(false_classifications, rows, cols, fig_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18b73b2-82f6-497f-af4e-da7e6f8d881a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
